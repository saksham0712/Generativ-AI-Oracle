<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Complete Notes: Oracle Generative AI Certification - LLMs</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1, h2, h3 { color: #2c3e50; }
    img { max-width: 100%; margin: 10px 0; }
    table { width: 100%; border-collapse: collapse; margin: 15px 0; }
    table, th, td { border: 1px solid #ccc; }
    th, td { padding: 8px; text-align: left; }
    pre { background: #f4f4f4; padding: 10px; overflow-x: auto; }
    hr { margin: 30px 0; }
  </style>
</head>
<body>
  <h1>Complete Notes: Oracle Generative AI Certification - Large Language Models</h1>

  <h2>Course Overview</h2>
  <p>This comprehensive guide covers Module 1 of Oracle's Generative AI Certification, focusing on Large Language Models (LLMs). It provides a technical understanding of LLMs, prompting techniques, training and decoding processes, dangers, and cutting-edge topics.</p>

  <h2>Lecture 1: Introduction to LLMs</h2>
  <h3>What is a Language Model?</h3>
  <p>Language models are <strong>probabilistic models of text</strong> that compute probability distributions over vocabulary words.</p>

  <h3>Probability Distribution Example</h3>
  <p>Given: "I wrote to the zoo to send me a pet. They sent me a ___"</p>
  <img src="https://user-gen-media-assets.s3.amazonaws.com/gemini_images/496d14f4-233d-4949-9c4d-c41f54941832.png" alt="Probability Distribution Example">
  <ul>
    <li>dog: 40%</li>
    <li>cat: 25%</li>
    <li>elephant: 15%</li>
    <li>panda: 10%</li>
  </ul>

  <h3>LLM Text Generation Process</h3>
  <img src="https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/93df6f6ff84336bc811a490a52664527/2dfab69b-f409-4198-bae2-3c0549290870/df9399f3.png" alt="LLM Text Generation Flow">

  <hr>

  <h2>Lecture 2: LLM Architectures</h2>
  <img src="https://user-gen-media-assets.s3.amazonaws.com/gemini_images/3388b885-371a-47d2-bd9d-d7a7cd7d550d.png" alt="LLM Architecture Overview">

  <h3>Architecture Types</h3>
  <ul>
    <li><strong>Encoders</strong>: Text embedding â†’ vector representations</li>
    <li><strong>Decoders</strong>: Iterative text generation</li>
    <li><strong>Encoder-Decoder</strong>: Sequence-to-sequence tasks</li>
  </ul>

  <h3>Transformer Foundation</h3>
  <p>Based on "Attention Is All You Need" (2017). Transformed NLP with attention mechanisms.</p>

  <h3>Architecture-Task Mapping</h3>
  <table>
    <tr><th>Task</th><th>Encoder</th><th>Decoder</th><th>Encoder-Decoder</th></tr>
    <tr><td>Embedding</td><td>Yes</td><td>No</td><td>Partial</td></tr>
    <tr><td>Text Generation</td><td>No</td><td>Yes</td><td>Yes</td></tr>
    <tr><td>Translation</td><td>No</td><td>Possible</td><td>Yes</td></tr>
    <tr><td>Classification</td><td>Yes</td><td>Possible</td><td>Possible</td></tr>
  </table>

  <hr>

  <h2>Lecture 3: Prompting & Prompt Engineering</h2>
  <h3>Prompting</h3>
  <p>Alter input to change output distribution. E.g., adding "little" shifts probabilities.</p>

  <h3>Chain-of-Thought Prompting</h3>
  <img src="https://user-gen-media-assets.s3.amazonaws.com/gemini_images/951b39d7-311f-4b3a-bd57-1d080ced105b.png" alt="Chain-of-Thought Prompting">

  <h3>Prompt Examples</h3>
  <pre>1 + 2 = 3
5 + 6 = 11
1 + 8 = ___</pre>

  <hr>

  <h2>Lecture 4: Prompt Injection</h2>
  <img src="https://user-gen-media-assets.s3.amazonaws.com/gemini_images/37d4959d-efb5-4ea7-a9f9-8448afae0834.png" alt="Prompt Injection Attack">
  <p>Levels of severity from minor text disruption to executing malicious commands.</p>

  <hr>

  <h2>Lecture 5: Training Methods</h2>
  <img src="https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/93df6f6ff84336bc811a490a52664527/af1538a4-5df9-44ec-88cf-6aa3bf29b20f/b68a6e17.png" alt="Training Methods Comparison">

  <hr>

  <h2>Lecture 6: Decoding Strategies</h2>
  <h3>Temperature Effects</h3>
  <img src="https://user-gen-media-assets.s3.amazonaws.com/gemini_images/115dd773-6f23-4852-80e9-5732e2c168b7.png" alt="Temperature Effects on Distribution">

  <hr>

  <h2>Lecture 7: Hallucination</h2>
  <img src="https://user-gen-media-assets.s3.amazonaws.com/gemini_images/24fb0108-cb3b-4545-b038-ce8f8245d024.png" alt="Hallucination Detection System">

  <hr>

  <h2>Lecture 8: Applications</h2>
  <h3>RAG System Architecture</h3>
  <img src="https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/93df6f6ff84336bc811a490a52664527/d3a7770b-b876-4f4f-84d8-2d087381e550/c0d33fdc.png" alt="RAG Architecture">

  <h3>ReAct Framework</h3>
  <img src="https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/93df6f6ff84336bc811a490a52664527/e3fa57f9-3f47-480e-b86c-73b4dc2a61d6/dc67410b.png" alt="ReAct Framework Process">

</body>
</html>
